{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b84093ba",
   "metadata": {},
   "source": [
    "# ARC Task Analysis for Training Parameter Optimization\n",
    "\n",
    "This notebook analyzes ARC tasks to determine optimal training parameters and hyperparameters for TinyLM model training.\n",
    "\n",
    "## Analysis includes:\n",
    "- Token length distribution analysis\n",
    "- Grid size statistics\n",
    "- Memory requirement estimation\n",
    "- Optimal batch size and sequence length recommendations\n",
    "- Training parameter suggestions based on available hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88435738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and environment check\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c669bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository and setup (for Google Colab)\n",
    "!git clone https://github.com/CalebTalley2024/ARC-AGI-2.git\n",
    "%cd ARC-AGI-2\n",
    "# Checkout specific branch for consistency\n",
    "!git checkout vedant\n",
    "\n",
    "# Install package and dependencies\n",
    "!pip install -e .\n",
    "!pip install matplotlib seaborn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c6c1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd()\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import ARC modules\n",
    "from arc.io.loader import load_task, iter_tasks\n",
    "\n",
    "print(\"Successfully imported ARC modules\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Check data availability\n",
    "data_dir = project_root / \"data\" / \"raw\" / \"arc\"\n",
    "training_dir = data_dir / \"training\"\n",
    "eval_dir = data_dir / \"evaluation\"\n",
    "\n",
    "print(f\"Training data exists: {training_dir.exists()}\")\n",
    "print(f\"Evaluation data exists: {eval_dir.exists()}\")\n",
    "\n",
    "if training_dir.exists():\n",
    "    training_files = list(training_dir.glob(\"*.json\"))\n",
    "    print(f\"Training files found: {len(training_files)}\")\n",
    "\n",
    "if eval_dir.exists():\n",
    "    eval_files = list(eval_dir.glob(\"*.json\"))\n",
    "    print(f\"Evaluation files found: {len(eval_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8ae1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze ARC tasks\n",
    "def load_arc_tasks(split=\"training\", max_tasks=None):\n",
    "    \"\"\"Load ARC tasks from specified split.\"\"\"\n",
    "    tasks = []\n",
    "    task_files = []\n",
    "    \n",
    "    if split == \"training\":\n",
    "        task_dir = training_dir\n",
    "    else:\n",
    "        task_dir = eval_dir\n",
    "    \n",
    "    if not task_dir.exists():\n",
    "        print(f\"ERROR: {task_dir} does not exist\")\n",
    "        return [], []\n",
    "    \n",
    "    json_files = list(task_dir.glob(\"*.json\"))[:max_tasks] if max_tasks else list(task_dir.glob(\"*.json\"))\n",
    "    \n",
    "    print(f\"Loading {len(json_files)} tasks from {split} split...\")\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            task_data = load_task(json_file)\n",
    "            tasks.append(task_data)\n",
    "            task_files.append(json_file.name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {json_file}: {e}\")\n",
    "    \n",
    "    print(f\"Successfully loaded {len(tasks)} tasks\")\n",
    "    return tasks, task_files\n",
    "\n",
    "# Load training tasks for analysis\n",
    "training_tasks, training_files = load_arc_tasks(\"training\")\n",
    "print(f\"Loaded {len(training_tasks)} training tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14d0ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid size analysis\n",
    "def analyze_grid_sizes(tasks):\n",
    "    \"\"\"Analyze grid sizes across all tasks.\"\"\"\n",
    "    grid_stats = {\n",
    "        'input_heights': [],\n",
    "        'input_widths': [],\n",
    "        'output_heights': [],\n",
    "        'output_widths': [],\n",
    "        'input_areas': [],\n",
    "        'output_areas': []\n",
    "    }\n",
    "    \n",
    "    for task in tasks:\n",
    "        # Analyze training pairs\n",
    "        for train_pair in task.get('train', []):\n",
    "            input_grid = train_pair['input']\n",
    "            output_grid = train_pair['output']\n",
    "            \n",
    "            # Input grid stats\n",
    "            in_h, in_w = len(input_grid), len(input_grid[0]) if input_grid else 0\n",
    "            grid_stats['input_heights'].append(in_h)\n",
    "            grid_stats['input_widths'].append(in_w)\n",
    "            grid_stats['input_areas'].append(in_h * in_w)\n",
    "            \n",
    "            # Output grid stats\n",
    "            out_h, out_w = len(output_grid), len(output_grid[0]) if output_grid else 0\n",
    "            grid_stats['output_heights'].append(out_h)\n",
    "            grid_stats['output_widths'].append(out_w)\n",
    "            grid_stats['output_areas'].append(out_h * out_w)\n",
    "        \n",
    "        # Analyze test inputs\n",
    "        for test_pair in task.get('test', []):\n",
    "            input_grid = test_pair['input']\n",
    "            in_h, in_w = len(input_grid), len(input_grid[0]) if input_grid else 0\n",
    "            grid_stats['input_heights'].append(in_h)\n",
    "            grid_stats['input_widths'].append(in_w)\n",
    "            grid_stats['input_areas'].append(in_h * in_w)\n",
    "    \n",
    "    return grid_stats\n",
    "\n",
    "# Analyze grid sizes\n",
    "print(\"Analyzing grid sizes...\")\n",
    "grid_stats = analyze_grid_sizes(training_tasks)\n",
    "\n",
    "print(\"\\nGrid Size Statistics:\")\n",
    "for key, values in grid_stats.items():\n",
    "    if values:\n",
    "        arr = np.array(values)\n",
    "        print(f\"{key}:\")\n",
    "        print(f\"  Min: {arr.min()}, Max: {arr.max()}\")\n",
    "        print(f\"  Mean: {arr.mean():.1f}, Median: {np.median(arr):.1f}\")\n",
    "        print(f\"  95th percentile: {np.percentile(arr, 95):.1f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a073543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token length analysis (mock implementation since serialize module needs completion)\n",
    "def estimate_token_length(grid, mode='row'):\n",
    "    \"\"\"\n",
    "    Estimate token length for a grid based on serialization mode.\n",
    "    This is a rough estimate since the actual tokenization depends on the implementation.\n",
    "    \"\"\"\n",
    "    if not grid:\n",
    "        return 0\n",
    "    \n",
    "    height = len(grid)\n",
    "    width = len(grid[0]) if grid else 0\n",
    "    area = height * width\n",
    "    \n",
    "    # Rough estimation based on serialization format:\n",
    "    # - Grid cells: area tokens\n",
    "    # - Metadata: ~5-10 tokens (height, width, separators)\n",
    "    # - Special tokens: ~3-5 tokens\n",
    "    \n",
    "    estimated_tokens = area + 15  # Grid + metadata + special tokens\n",
    "    return estimated_tokens\n",
    "\n",
    "def analyze_token_lengths(tasks, mode='row'):\n",
    "    \"\"\"Analyze estimated token lengths for training pairs.\"\"\"\n",
    "    sequence_lengths = []\n",
    "    individual_grid_lengths = []\n",
    "    \n",
    "    for task in tasks:\n",
    "        for train_pair in task.get('train', []):\n",
    "            input_grid = train_pair['input']\n",
    "            output_grid = train_pair['output']\n",
    "            \n",
    "            # Estimate tokens for individual grids\n",
    "            input_tokens = estimate_token_length(input_grid, mode)\n",
    "            output_tokens = estimate_token_length(output_grid, mode)\n",
    "            \n",
    "            individual_grid_lengths.extend([input_tokens, output_tokens])\n",
    "            \n",
    "            # Estimate total sequence length (input + output + separators)\n",
    "            total_tokens = input_tokens + output_tokens + 10  # Additional separators and special tokens\n",
    "            sequence_lengths.append(total_tokens)\n",
    "    \n",
    "    return np.array(sequence_lengths), np.array(individual_grid_lengths)\n",
    "\n",
    "# Analyze token lengths\n",
    "print(\"Analyzing estimated token lengths...\")\n",
    "seq_lengths, grid_lengths = analyze_token_lengths(training_tasks)\n",
    "\n",
    "print(\"\\nToken Length Analysis:\")\n",
    "print(\"Individual Grid Lengths:\")\n",
    "print(f\"  Min: {grid_lengths.min()}\")\n",
    "print(f\"  Max: {grid_lengths.max()}\")\n",
    "print(f\"  Mean: {grid_lengths.mean():.1f}\")\n",
    "print(f\"  95th percentile: {np.percentile(grid_lengths, 95):.1f}\")\n",
    "\n",
    "print(\"\\nSequence Lengths (input + output + separators):\")\n",
    "print(f\"  Min: {seq_lengths.min()}\")\n",
    "print(f\"  Max: {seq_lengths.max()}\")\n",
    "print(f\"  Mean: {seq_lengths.mean():.1f}\")\n",
    "print(f\"  95th percentile: {np.percentile(seq_lengths, 95):.1f}\")\n",
    "\n",
    "# Analyze impact of different max_len thresholds\n",
    "thresholds = [512, 1024, 2048, 4096]\n",
    "print(\"\\nImpact of max_len thresholds:\")\n",
    "for threshold in thresholds:\n",
    "    included = (seq_lengths <= threshold).sum()\n",
    "    percentage = (included / len(seq_lengths)) * 100\n",
    "    print(f\"  max_len={threshold}: {included}/{len(seq_lengths)} examples ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d835c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Grid size distributions\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.hist(grid_stats['input_areas'], bins=30, alpha=0.7, label='Input areas')\n",
    "plt.hist(grid_stats['output_areas'], bins=30, alpha=0.7, label='Output areas')\n",
    "plt.xlabel('Grid Area (cells)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Grid Area Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.hist(grid_stats['input_heights'], bins=30, alpha=0.7, label='Input heights')\n",
    "plt.hist(grid_stats['output_heights'], bins=30, alpha=0.7, label='Output heights')\n",
    "plt.xlabel('Height')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Grid Height Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.hist(grid_stats['input_widths'], bins=30, alpha=0.7, label='Input widths')\n",
    "plt.hist(grid_stats['output_widths'], bins=30, alpha=0.7, label='Output widths')\n",
    "plt.xlabel('Width')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Grid Width Distribution')\n",
    "plt.legend()\n",
    "\n",
    "# Token length distributions\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.hist(seq_lengths, bins=50, alpha=0.7, color='green')\n",
    "plt.xlabel('Estimated Sequence Length (tokens)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Sequence Length Distribution')\n",
    "plt.axvline(1024, color='red', linestyle='--', label='max_len=1024')\n",
    "plt.axvline(2048, color='orange', linestyle='--', label='max_len=2048')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.hist(grid_lengths, bins=50, alpha=0.7, color='blue')\n",
    "plt.xlabel('Individual Grid Length (tokens)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Individual Grid Token Length')\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "# Cumulative distribution\n",
    "sorted_lengths = np.sort(seq_lengths)\n",
    "y_vals = np.arange(1, len(sorted_lengths) + 1) / len(sorted_lengths)\n",
    "plt.plot(sorted_lengths, y_vals, linewidth=2)\n",
    "plt.xlabel('Sequence Length (tokens)')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('Cumulative Distribution of Sequence Lengths')\n",
    "plt.axvline(1024, color='red', linestyle='--', label='max_len=1024')\n",
    "plt.axvline(2048, color='orange', linestyle='--', label='max_len=2048')\n",
    "plt.axvline(4096, color='purple', linestyle='--', label='max_len=4096')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87356511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory estimation and optimization recommendations\n",
    "def estimate_memory_usage(batch_size, max_len, d_model=448, vocab_size=1000):\n",
    "    \"\"\"\n",
    "    Estimate GPU memory usage for training.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Training batch size\n",
    "        max_len: Maximum sequence length\n",
    "        d_model: Model dimension\n",
    "        vocab_size: Vocabulary size\n",
    "    \n",
    "    Returns:\n",
    "        Estimated memory usage in GB\n",
    "    \"\"\"\n",
    "    # Model parameters (rough estimate)\n",
    "    # Embeddings: vocab_size * d_model + max_len * d_model\n",
    "    # Transformer blocks: approximately 12 * d_model^2 per layer (for 8 layers)\n",
    "    # Output head: d_model * vocab_size\n",
    "    \n",
    "    embedding_params = (vocab_size + max_len) * d_model\n",
    "    transformer_params = 8 * 12 * d_model * d_model  # 8 layers\n",
    "    output_params = d_model * vocab_size\n",
    "    \n",
    "    total_params = embedding_params + transformer_params + output_params\n",
    "    \n",
    "    # Memory for parameters (4 bytes per float32)\n",
    "    param_memory = total_params * 4 / 1e9  # GB\n",
    "    \n",
    "    # Memory for activations (rough estimate)\n",
    "    # Forward pass: batch_size * max_len * d_model * layers * 4 bytes\n",
    "    activation_memory = batch_size * max_len * d_model * 8 * 4 / 1e9  # GB\n",
    "    \n",
    "    # Gradients (same as parameters)\n",
    "    gradient_memory = param_memory\n",
    "    \n",
    "    # Optimizer states (AdamW: 2x parameters)\n",
    "    optimizer_memory = param_memory * 2\n",
    "    \n",
    "    total_memory = param_memory + activation_memory + gradient_memory + optimizer_memory\n",
    "    \n",
    "    return {\n",
    "        'parameters': param_memory,\n",
    "        'activations': activation_memory,\n",
    "        'gradients': gradient_memory,\n",
    "        'optimizer': optimizer_memory,\n",
    "        'total': total_memory\n",
    "    }\n",
    "\n",
    "def get_hardware_recommendations():\n",
    "    \"\"\"Get training recommendations based on available hardware.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "    else:\n",
    "        gpu_memory = 0\n",
    "        gpu_name = \"CPU\"\n",
    "    \n",
    "    print(f\"Hardware: {gpu_name}\")\n",
    "    print(f\"Available memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    # Generate recommendations\n",
    "    configs = []\n",
    "    \n",
    "    for max_len in [512, 1024, 2048, 4096]:\n",
    "        for batch_size in [4, 8, 16, 32]:\n",
    "            memory_est = estimate_memory_usage(batch_size, max_len)\n",
    "            \n",
    "            if memory_est['total'] < gpu_memory * 0.9:  # 90% of available memory\n",
    "                percentage_data = (seq_lengths <= max_len).mean() * 100\n",
    "                configs.append({\n",
    "                    'max_len': max_len,\n",
    "                    'batch_size': batch_size,\n",
    "                    'memory_gb': memory_est['total'],\n",
    "                    'data_coverage': percentage_data\n",
    "                })\n",
    "    \n",
    "    return configs\n",
    "\n",
    "# Get recommendations\n",
    "print(\"Generating hardware-specific recommendations...\")\n",
    "recommendations = get_hardware_recommendations()\n",
    "\n",
    "print(\"\\nRecommended Configurations:\")\n",
    "print(\"max_len | batch_size | memory (GB) | data coverage (%)\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for config in recommendations[-10:]:  # Show top 10 configurations\n",
    "    print(f\"{config['max_len']:7d} | {config['batch_size']:10d} | {config['memory_gb']:11.2f} | {config['data_coverage']:13.1f}\")\n",
    "\n",
    "# Find optimal configuration\n",
    "if recommendations:\n",
    "    # Prefer configurations with high data coverage and reasonable memory usage\n",
    "    best_config = max(recommendations, key=lambda x: x['data_coverage'] - x['memory_gb'] * 10)\n",
    "    \n",
    "    print(f\"\\nRecommended optimal configuration:\")\n",
    "    print(f\"  max_len: {best_config['max_len']}\")\n",
    "    print(f\"  batch_size: {best_config['batch_size']}\")\n",
    "    print(f\"  Estimated memory usage: {best_config['memory_gb']:.2f} GB\")\n",
    "    print(f\"  Data coverage: {best_config['data_coverage']:.1f}%\")\n",
    "else:\n",
    "    print(\"\\nWARNING: No configurations fit in available memory. Consider:\")\n",
    "    print(\"  - Using a smaller model\")\n",
    "    print(\"  - Reducing batch size further\")\n",
    "    print(\"  - Using gradient accumulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d154c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task complexity analysis\n",
    "def analyze_task_complexity(tasks):\n",
    "    \"\"\"Analyze various complexity metrics of ARC tasks.\"\"\"\n",
    "    complexity_stats = {\n",
    "        'num_train_pairs': [],\n",
    "        'num_test_pairs': [],\n",
    "        'color_diversity': [],\n",
    "        'shape_changes': [],\n",
    "        'size_changes': []\n",
    "    }\n",
    "    \n",
    "    for task in tasks:\n",
    "        # Number of training and test pairs\n",
    "        complexity_stats['num_train_pairs'].append(len(task.get('train', [])))\n",
    "        complexity_stats['num_test_pairs'].append(len(task.get('test', [])))\n",
    "        \n",
    "        # Analyze color diversity and transformations\n",
    "        for train_pair in task.get('train', []):\n",
    "            input_grid = train_pair['input']\n",
    "            output_grid = train_pair['output']\n",
    "            \n",
    "            # Count unique colors\n",
    "            input_colors = set()\n",
    "            output_colors = set()\n",
    "            \n",
    "            for row in input_grid:\n",
    "                input_colors.update(row)\n",
    "            for row in output_grid:\n",
    "                output_colors.update(row)\n",
    "            \n",
    "            complexity_stats['color_diversity'].append(len(input_colors | output_colors))\n",
    "            \n",
    "            # Check if grid size changes\n",
    "            input_shape = (len(input_grid), len(input_grid[0]) if input_grid else 0)\n",
    "            output_shape = (len(output_grid), len(output_grid[0]) if output_grid else 0)\n",
    "            \n",
    "            complexity_stats['shape_changes'].append(input_shape != output_shape)\n",
    "            complexity_stats['size_changes'].append(\n",
    "                (input_shape[0] * input_shape[1]) != (output_shape[0] * output_shape[1])\n",
    "            )\n",
    "    \n",
    "    return complexity_stats\n",
    "\n",
    "# Analyze task complexity\n",
    "print(\"Analyzing task complexity...\")\n",
    "complexity_stats = analyze_task_complexity(training_tasks)\n",
    "\n",
    "print(\"\\nTask Complexity Statistics:\")\n",
    "for key, values in complexity_stats.items():\n",
    "    if key in ['shape_changes', 'size_changes']:\n",
    "        # Boolean values\n",
    "        true_count = sum(values)\n",
    "        percentage = (true_count / len(values)) * 100 if values else 0\n",
    "        print(f\"{key}: {true_count}/{len(values)} ({percentage:.1f}%)\")\n",
    "    else:\n",
    "        # Numeric values\n",
    "        if values:\n",
    "            arr = np.array(values)\n",
    "            print(f\"{key}:\")\n",
    "            print(f\"  Min: {arr.min()}, Max: {arr.max()}\")\n",
    "            print(f\"  Mean: {arr.mean():.1f}, Median: {np.median(arr):.1f}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2974824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameter recommendations\n",
    "def generate_training_recommendations(seq_lengths, gpu_memory_gb):\n",
    "    \"\"\"Generate comprehensive training recommendations.\"\"\"\n",
    "    \n",
    "    print(\"TRAINING HYPERPARAMETER RECOMMENDATIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Data coverage analysis\n",
    "    print(\"\\n1. SEQUENCE LENGTH ANALYSIS:\")\n",
    "    for max_len in [1024, 2048, 4096]:\n",
    "        coverage = (seq_lengths <= max_len).mean() * 100\n",
    "        excluded = len(seq_lengths) - (seq_lengths <= max_len).sum()\n",
    "        print(f\"   max_len={max_len}: {coverage:.1f}% coverage ({excluded} examples excluded)\")\n",
    "    \n",
    "    # Memory-based recommendations\n",
    "    print(f\"\\n2. MEMORY-BASED RECOMMENDATIONS (Available: {gpu_memory_gb:.1f}GB):\")\n",
    "    \n",
    "    memory_configs = [\n",
    "        (4096, 8, \"Maximum coverage, high memory\"),\n",
    "        (2048, 16, \"Good coverage, moderate memory\"),\n",
    "        (1024, 32, \"Lower coverage, efficient memory\"),\n",
    "        (1024, 16, \"Conservative, very stable\")\n",
    "    ]\n",
    "    \n",
    "    for max_len, batch_size, description in memory_configs:\n",
    "        memory_est = estimate_memory_usage(batch_size, max_len)\n",
    "        fits = \"YES\" if memory_est['total'] < gpu_memory_gb * 0.9 else \"NO\"\n",
    "        coverage = (seq_lengths <= max_len).mean() * 100\n",
    "        print(f\"   max_len={max_len}, batch_size={batch_size}: {memory_est['total']:.2f}GB - {fits} ({description})\")\n",
    "        print(f\"     Data coverage: {coverage:.1f}%\")\n",
    "    \n",
    "    # Learning rate recommendations\n",
    "    print(\"\\n3. LEARNING RATE RECOMMENDATIONS:\")\n",
    "    print(\"   Base LR: 3e-4 (good starting point)\")\n",
    "    print(\"   For smaller batches (<16): Consider 1e-4 to 3e-4\")\n",
    "    print(\"   For larger batches (>16): Consider 3e-4 to 6e-4\")\n",
    "    print(\"   Use warmup: 1000-2000 steps\")\n",
    "    print(\"   Consider cosine decay for longer training\")\n",
    "    \n",
    "    # Training duration recommendations\n",
    "    print(\"\\n4. TRAINING DURATION:\")\n",
    "    total_examples = len(seq_lengths)\n",
    "    print(f\"   Total training examples: {total_examples}\")\n",
    "    print(\"   Recommended steps:\")\n",
    "    \n",
    "    for batch_size in [8, 16, 32]:\n",
    "        epochs_10k = (10000 * batch_size) / total_examples\n",
    "        epochs_50k = (50000 * batch_size) / total_examples\n",
    "        print(f\"     Batch size {batch_size}:\")\n",
    "        print(f\"       10K steps = {epochs_10k:.1f} epochs\")\n",
    "        print(f\"       50K steps = {epochs_50k:.1f} epochs\")\n",
    "    \n",
    "    print(\"\\n5. MODEL SIZE RECOMMENDATIONS:\")\n",
    "    print(\"   d_model=448: Good balance (current)\")\n",
    "    print(\"   d_model=512: Slightly larger, more capacity\")\n",
    "    print(\"   d_model=384: Smaller, faster training\")\n",
    "    print(\"   Consider n_layers=6-8 (current: 8)\")\n",
    "    \n",
    "    print(\"\\n6. OPTIMIZATION RECOMMENDATIONS:\")\n",
    "    print(\"   - Use gradient clipping (1.0)\")\n",
    "    print(\"   - Save best model based on validation loss\")\n",
    "    print(\"   - Monitor training/validation loss ratio\")\n",
    "    print(\"   - Use mixed precision training (already implemented)\")\n",
    "    print(\"   - Consider gradient accumulation if memory constrained\")\n",
    "\n",
    "# Generate recommendations\n",
    "if torch.cuda.is_available():\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "else:\n",
    "    gpu_mem = 8.0  # Assume 8GB for CPU/unknown\n",
    "\n",
    "generate_training_recommendations(seq_lengths, gpu_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8137da2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export configuration for training\n",
    "def export_training_config(seq_lengths, gpu_memory_gb, output_file=\"optimal_config.json\"):\n",
    "    \"\"\"Export optimal training configuration as JSON.\"\"\"\n",
    "    \n",
    "    # Determine optimal configuration\n",
    "    if gpu_memory_gb > 20:  # A100/V100\n",
    "        optimal_config = {\n",
    "            \"max_len\": 2048,\n",
    "            \"batch_size\": 16,\n",
    "            \"learning_rate\": 3e-4,\n",
    "            \"steps\": 50000,\n",
    "            \"d_model\": 448\n",
    "        }\n",
    "    elif gpu_memory_gb > 14:  # T4\n",
    "        optimal_config = {\n",
    "            \"max_len\": 2048,\n",
    "            \"batch_size\": 12,\n",
    "            \"learning_rate\": 3e-4,\n",
    "            \"steps\": 50000,\n",
    "            \"d_model\": 448\n",
    "        }\n",
    "    else:  # Smaller GPUs\n",
    "        optimal_config = {\n",
    "            \"max_len\": 1024,\n",
    "            \"batch_size\": 16,\n",
    "            \"learning_rate\": 3e-4,\n",
    "            \"steps\": 30000,\n",
    "            \"d_model\": 384\n",
    "        }\n",
    "    \n",
    "    # Add analysis results\n",
    "    config_with_analysis = {\n",
    "        \"optimal_config\": optimal_config,\n",
    "        \"analysis_results\": {\n",
    "            \"total_examples\": len(seq_lengths),\n",
    "            \"max_sequence_length\": int(seq_lengths.max()),\n",
    "            \"mean_sequence_length\": float(seq_lengths.mean()),\n",
    "            \"data_coverage_1024\": float((seq_lengths <= 1024).mean() * 100),\n",
    "            \"data_coverage_2048\": float((seq_lengths <= 2048).mean() * 100),\n",
    "            \"data_coverage_4096\": float((seq_lengths <= 4096).mean() * 100),\n",
    "            \"gpu_memory_gb\": gpu_memory_gb\n",
    "        },\n",
    "        \"alternative_configs\": [\n",
    "            {\"name\": \"memory_efficient\", \"max_len\": 1024, \"batch_size\": 32, \"learning_rate\": 3e-4},\n",
    "            {\"name\": \"high_coverage\", \"max_len\": 4096, \"batch_size\": 8, \"learning_rate\": 2e-4},\n",
    "            {\"name\": \"balanced\", \"max_len\": 2048, \"batch_size\": 16, \"learning_rate\": 3e-4}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    import json\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(config_with_analysis, f, indent=2)\n",
    "    \n",
    "    print(f\"Configuration saved to {output_file}\")\n",
    "    print(f\"\\nOptimal configuration for your hardware:\")\n",
    "    for key, value in optimal_config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    return config_with_analysis\n",
    "\n",
    "# Export configuration\n",
    "config = export_training_config(seq_lengths, gpu_mem)\n",
    "\n",
    "print(f\"\\nSUMMARY:\")\n",
    "print(f\"- Analyzed {len(training_tasks)} ARC training tasks\")\n",
    "print(f\"- Found sequence lengths ranging from {seq_lengths.min()} to {seq_lengths.max()} tokens\")\n",
    "print(f\"- Generated hardware-specific recommendations for {gpu_mem:.1f}GB GPU\")\n",
    "print(f\"- Optimal configuration will train on {config['analysis_results']['data_coverage_2048']:.1f}% of data\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"1. Use the recommended configuration in your training script\")\n",
    "print(f\"2. Monitor training and validation loss\")\n",
    "print(f\"3. Adjust hyperparameters based on training progress\")\n",
    "print(f\"4. Consider data augmentation with grid transformations\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
