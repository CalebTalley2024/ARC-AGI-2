{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce1a54c",
   "metadata": {},
   "source": [
    "# TinyLM Training on Google Colab\n",
    "\n",
    "This notebook trains a TinyLM model on ARC-AGI data using Google Colab's GPU resources.\n",
    "\n",
    "## Setup Instructions:\n",
    "1. Upload this notebook to Google Colab\n",
    "2. Enable GPU runtime: Runtime → Change runtime type → GPU (T4/V100)\n",
    "3. Run all cells in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e157151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability and install requirements\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Please enable GPU runtime in Colab!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f4442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository from GitHub\n",
    "!git clone https://github.com/CalebTalley2024/ARC-AGI-2.git\n",
    "%cd ARC-AGI-2\n",
    "# Checkout specific branch for consistency\n",
    "!git checkout vedant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e43d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package and dependencies\n",
    "!pip install -e .\n",
    "!pip install tqdm transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb96a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path.cwd()\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import the training function\n",
    "from arc.models.train import train\n",
    "from arc.models.tiny_lm import TinyLMConfig\n",
    "\n",
    "print(\"Successfully imported training modules\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a707323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data directory structure\n",
    "data_dir = project_root / \"data\"\n",
    "print(\"Data directory contents:\")\n",
    "if data_dir.exists():\n",
    "    for item in data_dir.iterdir():\n",
    "        print(f\"  {item.name}\")\n",
    "        if item.is_dir():\n",
    "            for subitem in item.iterdir():\n",
    "                print(f\"    {subitem.name}\")\n",
    "                if subitem.name == \"arc\" and subitem.is_dir():\n",
    "                    for arcitem in subitem.iterdir():\n",
    "                        print(f\"      {arcitem.name}\")\n",
    "else:\n",
    "    print(\"  WARNING: Data directory not found!\")\n",
    "\n",
    "# Check if training data exists\n",
    "training_path = data_dir / \"raw\" / \"arc\" / \"training\"\n",
    "eval_path = data_dir / \"raw\" / \"arc\" / \"evaluation\"\n",
    "\n",
    "print(f\"\\nTraining data exists: {training_path.exists()}\")\n",
    "print(f\"Evaluation data exists: {eval_path.exists()}\")\n",
    "\n",
    "if training_path.exists():\n",
    "    training_files = list(training_path.glob(\"*.json\"))\n",
    "    print(f\"Number of training files: {len(training_files)}\")\n",
    "    \n",
    "if eval_path.exists():\n",
    "    eval_files = list(eval_path.glob(\"*.json\"))\n",
    "    print(f\"Number of evaluation files: {len(eval_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39402a9",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "Configure the training parameters. Adjust these based on your needs and available GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cfde04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "TRAINING_CONFIG = {\n",
    "    # Data paths\n",
    "    \"data_path\": str(data_dir / \"raw\" / \"arc\" / \"training\"),\n",
    "    \"model_dir\": \"./models/tinylm_checkpoints\",\n",
    "    \n",
    "    # Model parameters\n",
    "    \"d_model\": 448,        # Model dimension - adjust for GPU memory\n",
    "    \n",
    "    # Training parameters\n",
    "    \"steps\": 10_000,       # Number of training steps (reduce for testing)\n",
    "    \"batch_size\": 16,      # Batch size - adjust for GPU memory\n",
    "    \"learning_rate\": 3e-4, # Learning rate\n",
    "    \n",
    "    # Logging\n",
    "    \"save_every\": 1000,    # Save checkpoint every N steps\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Check GPU memory and suggest batch size\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    if gpu_memory < 8:\n",
    "        print(f\"\\nWARNING: GPU has {gpu_memory:.1f}GB memory. Consider reducing batch_size to 8 or 4\")\n",
    "        TRAINING_CONFIG[\"batch_size\"] = 8\n",
    "    elif gpu_memory >= 16:\n",
    "        print(f\"\\nSUCCESS: GPU has {gpu_memory:.1f}GB memory. You can increase batch_size to 32\")\n",
    "        TRAINING_CONFIG[\"batch_size\"] = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6290dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path(TRAINING_CONFIG[\"model_dir\"])\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Created output directory: {output_dir}\")\n",
    "\n",
    "# Quick test of model creation\n",
    "print(\"\\nTesting model creation...\")\n",
    "try:\n",
    "    from arc.serialize import VOCAB_SIZE\n",
    "    print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "    \n",
    "    # Create a test config\n",
    "    test_config = TinyLMConfig(vocab_size=VOCAB_SIZE, d_model=TRAINING_CONFIG[\"d_model\"])\n",
    "    print(f\"Model config created: {test_config.d_model}D model\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Error creating model: {e}\")\n",
    "    print(\"This might be due to missing implementations in serialize module\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a8e59c",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "\n",
    "**Note:** The current implementation has placeholder functions for data loading. This will train on empty data but demonstrates the training loop. You'll need to implement proper data loading for actual training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d7faf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "import time\n",
    "\n",
    "print(\"Starting TinyLM training...\")\n",
    "print(f\"Model will be saved to: {TRAINING_CONFIG['model_dir']}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Call the training function\n",
    "    train(\n",
    "        model_dir=TRAINING_CONFIG[\"model_dir\"],\n",
    "        data_path=TRAINING_CONFIG[\"data_path\"],\n",
    "        steps=TRAINING_CONFIG[\"steps\"],\n",
    "        bs=TRAINING_CONFIG[\"batch_size\"],\n",
    "        lr=TRAINING_CONFIG[\"learning_rate\"],\n",
    "        d_model=TRAINING_CONFIG[\"d_model\"]\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(f\"Training completed successfully!\")\n",
    "    print(f\"Training time: {training_time/60:.1f} minutes\")\n",
    "    print(f\"Models saved to: {TRAINING_CONFIG['model_dir']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Training failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45114c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training results\n",
    "import os\n",
    "\n",
    "model_dir = Path(TRAINING_CONFIG[\"model_dir\"])\n",
    "if model_dir.exists():\n",
    "    print(\"Training output files:\")\n",
    "    for file in sorted(model_dir.iterdir()):\n",
    "        if file.is_file():\n",
    "            size_mb = file.stat().st_size / (1024 * 1024)\n",
    "            print(f\"  {file.name}: {size_mb:.1f} MB\")\n",
    "    \n",
    "    # Check if best model exists\n",
    "    best_model = model_dir / \"best.pt\"\n",
    "    if best_model.exists():\n",
    "        print(f\"\\nBest model saved: {best_model}\")\n",
    "        # Load and display best model info\n",
    "        try:\n",
    "            import torch\n",
    "            checkpoint = torch.load(best_model, map_location='cpu')\n",
    "            if 'loss' in checkpoint:\n",
    "                print(f\"   Best loss: {checkpoint['loss']:.4f}\")\n",
    "            if 'cfg' in checkpoint:\n",
    "                cfg = checkpoint['cfg']\n",
    "                print(f\"   Model config: {cfg}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Could not load model info: {e}\")\n",
    "else:\n",
    "    print(\"No training output found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf257dd",
   "metadata": {},
   "source": [
    "## Model Testing (Optional)\n",
    "\n",
    "Test the trained model with a simple forward pass to ensure it's working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8432fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and test the best model\n",
    "model_path = model_dir / \"best.pt\"\n",
    "\n",
    "if model_path.exists():\n",
    "    print(\"Testing the trained model...\")\n",
    "    \n",
    "    try:\n",
    "        # Load the model\n",
    "        checkpoint = torch.load(model_path, map_location='cpu')\n",
    "        \n",
    "        # Recreate the model\n",
    "        from arc.models.tiny_lm import TinyLM, TinyLMConfig\n",
    "        from arc.serialize import VOCAB_SIZE\n",
    "        \n",
    "        cfg = TinyLMConfig(**checkpoint['cfg'])\n",
    "        model = TinyLM(cfg)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        \n",
    "        # Move to GPU if available\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        print(f\"Model loaded successfully on {device}\")\n",
    "        print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        \n",
    "        # Test with dummy input\n",
    "        batch_size, seq_len = 4, 16\n",
    "        dummy_input = torch.randint(0, VOCAB_SIZE, (batch_size, seq_len)).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(dummy_input)\n",
    "            print(f\"Forward pass successful!\")\n",
    "            print(f\"  Input shape: {dummy_input.shape}\")\n",
    "            print(f\"  Output shape: {output.shape}\")\n",
    "            print(f\"  Output range: [{output.min():.3f}, {output.max():.3f}]\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Model testing failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"No trained model found to test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917bf76c",
   "metadata": {},
   "source": [
    "## Download Trained Models\n",
    "\n",
    "Download the trained models to your local machine or save to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59942d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Download files directly (in Colab)\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "if model_dir.exists():\n",
    "    # Create a zip file of all models\n",
    "    zip_path = \"tinylm_models.zip\"\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "        for file in model_dir.iterdir():\n",
    "            if file.is_file():\n",
    "                zipf.write(file, file.name)\n",
    "    \n",
    "    print(f\"Created zip file: {zip_path}\")\n",
    "    print(\"Downloading...\")\n",
    "    files.download(zip_path)\n",
    "else:\n",
    "    print(\"No models to download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec53b995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Save to Google Drive (uncomment to use)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# \n",
    "# # Copy models to Google Drive\n",
    "# import shutil\n",
    "# drive_path = \"/content/drive/MyDrive/TinyLM_Models\"\n",
    "# if model_dir.exists():\n",
    "#     shutil.copytree(model_dir, drive_path, dirs_exist_ok=True)\n",
    "#     print(f\"Models saved to Google Drive: {drive_path}\")\n",
    "\n",
    "print(\"Training notebook complete!\")\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"   • Model architecture: TinyLM with {TRAINING_CONFIG['d_model']} dimensions\")\n",
    "print(f\"   • Training steps: {TRAINING_CONFIG['steps']:,}\")\n",
    "print(f\"   • Batch size: {TRAINING_CONFIG['batch_size']}\")\n",
    "print(f\"   • Learning rate: {TRAINING_CONFIG['learning_rate']}\")\n",
    "print(f\"   • Models saved to: {TRAINING_CONFIG['model_dir']}\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"   • Implement proper data loading in arc.io.load_task()\")\n",
    "print(\"   • Implement tokenization in arc.serialize.pack_example()\")\n",
    "print(\"   • Run evaluation on the trained model\")\n",
    "print(\"   • Experiment with different hyperparameters\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
