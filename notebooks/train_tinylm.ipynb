{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce1a54c",
   "metadata": {},
   "source": [
    "# TinyLM Training on Google Colab\n",
    "\n",
    "## This notebook trains a TinyLM model on ARC-AGI data using Google Colab's GPU resources.\n",
    "\n",
    "### Setup Instructions:\n",
    "1. Upload this notebook to Google Colab\n",
    "2. Enable GPU runtime: Runtime → Change runtime type → GPU (T4/V100)\n",
    "3. Run all cells in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e157151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.2\n",
      "CUDA available: False\n",
      "WARNING: No GPU detected. Please enable GPU runtime in Colab!\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability and install requirements\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Please enable GPU runtime in Colab!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f4442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository from GitHub\n",
    "!git clone https://github.com/CalebTalley2024/ARC-AGI-2.git\n",
    "%cd ARC-AGI-2\n",
    "# Checkout specific branch for consistency\n",
    "!git checkout vedant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e43d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install exact package versions for reproducibility\n",
    "!pip install --quiet \\\n",
    "    numpy==1.24.4 \\\n",
    "    matplotlib==3.7.5 \\\n",
    "    pandas==2.0.3 \\\n",
    "    scipy==1.10.1 \\\n",
    "    scikit-learn==1.3.2 \\\n",
    "    torch==2.2.2 \\\n",
    "    torchvision==0.17.2 \\\n",
    "    torchaudio==2.2.2 \\\n",
    "    transformers==4.46.3 \\\n",
    "    huggingface-hub==0.36.0 \\\n",
    "    tokenizers==0.20.3 \\\n",
    "    safetensors==0.5.3 \\\n",
    "    seaborn==0.13.2 \\\n",
    "    plotly==6.4.0 \\\n",
    "    tqdm==4.67.1 \\\n",
    "    pyyaml==6.0.3 \\\n",
    "    requests==2.32.4 \\\n",
    "    packaging==25.0 \\\n",
    "    jsonschema==4.23.0 \\\n",
    "    fastjsonschema==2.21.2 \\\n",
    "    jinja2==3.1.6 \\\n",
    "    markupsafe==2.1.5 \\\n",
    "    urllib3==2.2.3 \\\n",
    "    certifi==2025.10.5 \\\n",
    "    charset-normalizer==3.4.4 \\\n",
    "    idna==3.11 \\\n",
    "    python-dateutil==2.9.0.post0 \\\n",
    "    pytz==2025.2 \\\n",
    "    tzdata==2025.2 \\\n",
    "    six==1.17.0 \\\n",
    "    setuptools==75.3.2\n",
    "\n",
    "# Install the package in development mode\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb96a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported training modules and centralized constants\n",
      "Project root: /Users/vedanttibrewal/Documents/USC/lectures/sem_3/CSCI-544/project/ARC-AGI-2\n",
      "Available model sizes: ['tiny', 'small', 'medium', 'large']\n",
      "Available training profiles: ['debug', 'small_gpu', 'medium_gpu', 'large_gpu']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path (go up one level from notebooks)\n",
    "project_root = Path.cwd()\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import the training function and centralized constants\n",
    "from arc.models.train import train\n",
    "from arc.models.tiny_lm import TinyLMConfig\n",
    "from arc.utils.constants import (\n",
    "    MODEL_CONFIGS, \n",
    "    TRAINING_CONFIGS, \n",
    "    get_matched_configs,\n",
    "    estimate_model_parameters\n",
    ")\n",
    "\n",
    "print(\"Successfully imported training modules and centralized constants\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Available model sizes: {list(MODEL_CONFIGS.keys())}\")\n",
    "print(f\"Available training profiles: {list(TRAINING_CONFIGS.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a707323d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory contents:\n",
      "  processed\n",
      "    index.jsonl\n",
      "    .gitkeep\n",
      "    eval_tasks.json\n",
      "    dev_tasks.json\n",
      "  raw\n",
      "    arc\n",
      "      training\n",
      "      .gitkeep\n",
      "      evaluation.txt\n",
      "      training.txt\n",
      "      evaluation\n",
      "\n",
      "Training data exists: True\n",
      "Evaluation data exists: True\n",
      "Number of training files: 1000\n",
      "Number of evaluation files: 120\n"
     ]
    }
   ],
   "source": [
    "# Check data directory structure\n",
    "data_dir = project_root / \"data\"\n",
    "print(\"Data directory contents:\")\n",
    "if data_dir.exists():\n",
    "    for item in data_dir.iterdir():\n",
    "        print(f\"  {item.name}\")\n",
    "        if item.is_dir():\n",
    "            for subitem in item.iterdir():\n",
    "                print(f\"    {subitem.name}\")\n",
    "                if subitem.name == \"arc\" and subitem.is_dir():\n",
    "                    for arcitem in subitem.iterdir():\n",
    "                        print(f\"      {arcitem.name}\")\n",
    "else:\n",
    "    print(\"  WARNING: Data directory not found!\")\n",
    "\n",
    "# Check if training data exists\n",
    "training_path = data_dir / \"raw\" / \"arc\" / \"training\"\n",
    "eval_path = data_dir / \"raw\" / \"arc\" / \"evaluation\"\n",
    "\n",
    "print(f\"\\nTraining data exists: {training_path.exists()}\")\n",
    "print(f\"Evaluation data exists: {eval_path.exists()}\")\n",
    "\n",
    "if training_path.exists():\n",
    "    training_files = list(training_path.glob(\"*.json\"))\n",
    "    print(f\"Number of training files: {len(training_files)}\")\n",
    "    \n",
    "if eval_path.exists():\n",
    "    eval_files = list(eval_path.glob(\"*.json\"))\n",
    "    print(f\"Number of evaluation files: {len(eval_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39402a9",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "Configure the training parameters. Adjust these based on your needs and available GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cfde04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU detected! Using CPU-friendly minimal config\n",
      "\n",
      "Final Training Configuration:\n",
      "  Data path: /Users/vedanttibrewal/Documents/USC/lectures/sem_3/CSCI-544/project/ARC-AGI-2/data/raw/arc/training\n",
      "  Model dir: ./models/tinylm_checkpoints\n",
      "  Steps: 100\n",
      "  Learning rate: 0.0003\n",
      "  Weight decay: 0.01\n",
      "  Use AMP: True\n"
     ]
    }
   ],
   "source": [
    "# GPU-aware dynamic configuration selection\n",
    "def select_optimal_configs(gpu_memory_gb):\n",
    "    \"\"\"Select optimal model and training configs based on GPU memory.\"\"\"\n",
    "    \n",
    "    print(f\"GPU Memory: {gpu_memory_gb:.1f} GB\")\n",
    "    \n",
    "    # Select configs based on GPU memory\n",
    "    if gpu_memory_gb < 4:\n",
    "        model_size = 'tiny'\n",
    "        training_profile = 'debug'\n",
    "        print(\"Low GPU memory detected - using minimal config for testing\")\n",
    "    elif gpu_memory_gb < 8:\n",
    "        model_size = 'tiny'\n",
    "        training_profile = 'small_gpu'\n",
    "        print(\"Small GPU detected - using tiny model with memory optimization\")\n",
    "    elif gpu_memory_gb < 16:\n",
    "        model_size = 'small'\n",
    "        training_profile = 'medium_gpu'\n",
    "        print(\"Medium GPU detected - using small model\")\n",
    "    else:\n",
    "        model_size = 'medium'\n",
    "        training_profile = 'large_gpu'\n",
    "        print(\"Large GPU detected - using medium model for best performance\")\n",
    "    \n",
    "\n",
    "    model_config = \"small\"\n",
    "    training_config = \"small_gpu\"\n",
    "\n",
    "    # Get matched configurations\n",
    "    model_config, training_config = get_matched_configs(model_size, training_profile)\n",
    "    \n",
    "    # Display configuration info\n",
    "    param_count = estimate_model_parameters(model_config)\n",
    "    effective_batch_size = training_config['batch_size'] * training_config['grad_accumulation_steps']\n",
    "    \n",
    "    print(f\"\\nSelected Configuration:\")\n",
    "    print(f\"  Model: {model_size} ({param_count/1e6:.1f}M parameters)\")\n",
    "    print(f\"  Training profile: {training_profile}\")\n",
    "    print(f\"  Batch size: {training_config['batch_size']} (effective: {effective_batch_size})\")\n",
    "    print(f\"  Max sequence length: {training_config['max_sequence_length']}\")\n",
    "    print(f\"  Gradient accumulation: {training_config['grad_accumulation_steps']} steps\")\n",
    "    \n",
    "    return model_size, training_profile, model_config, training_config\n",
    "\n",
    "# Check GPU and select optimal configuration\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    selected_model_size, selected_training_profile, model_config, training_config = select_optimal_configs(gpu_memory)\n",
    "else:\n",
    "    print(\"No GPU detected! Using CPU-friendly minimal config\")\n",
    "    selected_model_size = 'tiny'\n",
    "    selected_training_profile = 'debug'\n",
    "    model_config, training_config = get_matched_configs(selected_model_size, selected_training_profile)\n",
    "\n",
    "# Training configuration for this session\n",
    "TRAINING_CONFIG = {\n",
    "    # Data paths\n",
    "    \"data_path\": str(data_dir / \"raw\" / \"arc\" / \"training\"),\n",
    "    \"model_dir\": \"./models/tinylm_checkpoints\",\n",
    "    \n",
    "    # Use centralized configurations\n",
    "    \"model_config\": model_config,\n",
    "    \"training_config\": training_config,\n",
    "    \n",
    "    # Store selected profiles for use in training\n",
    "    \"selected_model_size\": selected_model_size,\n",
    "    \"selected_training_profile\": selected_training_profile,\n",
    "}\n",
    "\n",
    "print(f\"\\nFinal Training Configuration:\")\n",
    "print(f\"  Model size: {selected_model_size}\")\n",
    "print(f\"  Training profile: {selected_training_profile}\")\n",
    "print(f\"  Data path: {TRAINING_CONFIG['data_path']}\")\n",
    "print(f\"  Model dir: {TRAINING_CONFIG['model_dir']}\")\n",
    "print(f\"  Steps: {training_config['steps']:,}\")\n",
    "print(f\"  Learning rate: {training_config['learning_rate']}\")\n",
    "print(f\"  Weight decay: {training_config['weight_decay']}\")\n",
    "print(f\"  Use AMP: {training_config['use_amp']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6290dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path(TRAINING_CONFIG[\"model_dir\"])\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Created output directory: {output_dir}\")\n",
    "\n",
    "# Test model creation with centralized config\n",
    "print(\"\\nTesting model creation with selected configuration...\")\n",
    "try:\n",
    "    from arc.utils.constants import VOCAB_SIZE\n",
    "    print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "    \n",
    "    # Create model with selected configuration\n",
    "    model_cfg = TRAINING_CONFIG[\"model_config\"]\n",
    "    test_config = TinyLMConfig(**model_cfg)\n",
    "    print(f\"Model config created successfully\")\n",
    "    print(f\"  Architecture: {test_config.d_model}D, {test_config.n_layers} layers, {test_config.n_heads} heads\")\n",
    "    print(f\"  Parameters: ~{estimate_model_parameters(model_cfg)/1e6:.1f}M\")\n",
    "    print(f\"  Max sequence length: {test_config.max_len}\")\n",
    "    \n",
    "    # Display memory usage estimate\n",
    "    param_size_mb = estimate_model_parameters(model_cfg) * 4 / 1e6  # 4 bytes per float32\n",
    "    training_cfg = TRAINING_CONFIG[\"training_config\"]\n",
    "    batch_memory_mb = (training_cfg['batch_size'] * training_cfg['max_sequence_length'] * \n",
    "                      model_cfg['d_model'] * 4) / 1e6\n",
    "    \n",
    "    print(f\"\\nMemory Estimates:\")\n",
    "    print(f\"  Model size: ~{param_size_mb:.0f} MB\")\n",
    "    print(f\"  Batch memory: ~{batch_memory_mb:.0f} MB\")\n",
    "    print(f\"  Total training memory: ~{param_size_mb + batch_memory_mb*3:.0f} MB (estimated)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating model: {e}\")\n",
    "    print(\"This might indicate missing implementations in serialize module\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4e3c515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data loading...\n",
      "Loading tasks from: /Users/vedanttibrewal/Documents/USC/lectures/sem_3/CSCI-544/project/ARC-AGI-2/data/raw/arc/training\n",
      "Found 1000 JSON files\n",
      "Loaded task from /Users/vedanttibrewal/Documents/USC/lectures/sem_3/CSCI-544/project/ARC-AGI-2/data/raw/arc/training/a85d4709.json: 4 training examples\n",
      "Loaded task from /Users/vedanttibrewal/Documents/USC/lectures/sem_3/CSCI-544/project/ARC-AGI-2/data/raw/arc/training/c8cbb738.json: 3 training examples\n",
      "Loaded task from /Users/vedanttibrewal/Documents/USC/lectures/sem_3/CSCI-544/project/ARC-AGI-2/data/raw/arc/training/8e1813be.json: 3 training examples\n",
      "\n",
      "Testing dataset creation...\n",
      "Dataset created successfully with 10 examples\n",
      "First example - Input shape: torch.Size([106]), Target shape: torch.Size([106])\n",
      "Input range: [1, 82], Target range: [2, 82]\n"
     ]
    }
   ],
   "source": [
    "# Test data loading before training\n",
    "import sys\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from arc.io.loader import load_tasks\n",
    "from arc.models.train import ArcPairsDataset\n",
    "\n",
    "print(\"Testing data loading...\")\n",
    "data_path = str(data_dir / \"raw\" / \"arc\" / \"training\")\n",
    "print(f\"Loading tasks from: {data_path}\")\n",
    "\n",
    "try:\n",
    "    # Load a few tasks to test\n",
    "    import glob\n",
    "    json_files = glob.glob(f\"{data_path}/*.json\")\n",
    "    print(f\"Found {len(json_files)} JSON files\")\n",
    "    \n",
    "    # Test loading just first few tasks\n",
    "    test_files = json_files[:3]\n",
    "    tasks = []\n",
    "    for file in test_files:\n",
    "        from arc.io.loader import load_task\n",
    "        task = load_task(file)\n",
    "        tasks.append(task)\n",
    "        print(f\"Loaded task from {file}: {len(task['train'])} training examples\")\n",
    "    \n",
    "    # Test dataset creation\n",
    "    print(\"\\nTesting dataset creation...\")\n",
    "    ds = ArcPairsDataset(tasks, max_len=512)  # Use smaller max_len for testing\n",
    "    print(f\"Dataset created successfully with {len(ds)} examples\")\n",
    "    \n",
    "    if len(ds) > 0:\n",
    "        # Test getting first example\n",
    "        x, y = ds[0]\n",
    "        print(f\"First example - Input shape: {x.shape}, Target shape: {y.shape}\")\n",
    "        print(f\"Input range: [{x.min()}, {x.max()}], Target range: [{y.min()}, {y.max()}]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in data loading: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c972937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing checkpoints and resume capability\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def check_checkpoint_info(model_dir):\n",
    "    \"\"\"Check what checkpoints are available for resuming.\"\"\"\n",
    "    model_path = Path(model_dir)\n",
    "    \n",
    "    # Check for best.pt\n",
    "    best_checkpoint = model_path / \"best.pt\"\n",
    "    final_checkpoint = model_path / \"final.pt\"\n",
    "    \n",
    "    print(\"Checkpoint Status:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if best_checkpoint.exists():\n",
    "        try:\n",
    "            checkpoint = torch.load(best_checkpoint, map_location='cpu')\n",
    "            print(f\"✓ Found best.pt checkpoint:\")\n",
    "            print(f\"  - Best loss: {checkpoint.get('loss', 'N/A'):.4f}\")\n",
    "            print(f\"  - Last step: {checkpoint.get('step', 'N/A')}\")\n",
    "            print(f\"  - Learning rate: {checkpoint.get('lr', 'N/A')}\")\n",
    "            \n",
    "            if 'training_config' in checkpoint:\n",
    "                train_cfg = checkpoint['training_config']\n",
    "                print(f\"  - Total steps: {train_cfg.get('steps', 'N/A')}\")\n",
    "                print(f\"  - Batch size: {train_cfg.get('batch_size', 'N/A')}\")\n",
    "                steps_remaining = train_cfg.get('steps', 0) - checkpoint.get('step', 0) - 1\n",
    "                print(f\"  - Steps remaining: {max(0, steps_remaining)}\")\n",
    "            \n",
    "            # Check if training was completed\n",
    "            if checkpoint.get('training_completed', False):\n",
    "                print(\"  - Status: Training completed\")\n",
    "            else:\n",
    "                print(\"  - Status: Training can be resumed\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Found best.pt but couldn't load: {e}\")\n",
    "    else:\n",
    "        print(\"✗ No best.pt checkpoint found\")\n",
    "    \n",
    "    if final_checkpoint.exists():\n",
    "        try:\n",
    "            checkpoint = torch.load(final_checkpoint, map_location='cpu')\n",
    "            print(f\"\\n✓ Found final.pt checkpoint:\")\n",
    "            if checkpoint.get('training_completed', False):\n",
    "                print(\"  - Training completed successfully\")\n",
    "            else:\n",
    "                print(\"  - Training may have been interrupted\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n✗ Found final.pt but couldn't load: {e}\")\n",
    "    \n",
    "    # Check for regular checkpoints\n",
    "    checkpoint_files = list(model_path.glob(\"ckpt_*.pt\"))\n",
    "    if checkpoint_files:\n",
    "        print(f\"\\n✓ Found {len(checkpoint_files)} regular checkpoints:\")\n",
    "        for ckpt_file in sorted(checkpoint_files)[-3:]:  # Show last 3\n",
    "            print(f\"  - {ckpt_file.name}\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# Check current checkpoint status\n",
    "checkpoint_dir = TRAINING_CONFIG[\"model_dir\"]\n",
    "check_checkpoint_info(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c99daf",
   "metadata": {},
   "source": [
    "## Checkpoint Resuming\n",
    "\n",
    "This notebook now supports automatic resuming from the `best.pt` checkpoint file. Here's how it works:\n",
    "\n",
    "### Automatic Resuming Features:\n",
    "- **Model State**: Automatically loads the best model weights\n",
    "- **Optimizer State**: Restores AdamW optimizer momentum and parameters  \n",
    "- **Training Progress**: Resumes from the exact step where training stopped\n",
    "- **Best Loss Tracking**: Continues tracking the best validation loss\n",
    "- **Mixed Precision**: Restores AMP scaler state for consistent training\n",
    "\n",
    "### Checkpoint Contents:\n",
    "Each checkpoint now contains:\n",
    "- `model`: Model state dictionary\n",
    "- `cfg`: Model configuration  \n",
    "- `loss`: Best loss achieved\n",
    "- `step`: Current training step\n",
    "- `optimizer`: Optimizer state (momentum, etc.)\n",
    "- `scaler`: Mixed precision scaler state\n",
    "- `training_config`: Training hyperparameters\n",
    "- `model_config`: Model architecture settings\n",
    "\n",
    "### Usage:\n",
    "- Set `RESUME_TRAINING = True` to enable resuming (default)\n",
    "- Set `RESUME_TRAINING = False` to start from scratch\n",
    "- The system automatically detects and loads `best.pt` if available\n",
    "- Use the checkpoint management utilities below for advanced operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a8e59c",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "\n",
    "**Note:** The current implementation has placeholder functions for data loading. This will train on empty data but demonstrates the training loop. You'll need to implement proper data loading for actual training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d7faf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TinyLM training with centralized configuration...\n",
      "Model will be saved to: ./models/tinylm_checkpoints\n",
      "============================================================\n",
      "CONFIGURATION SUMMARY:\n",
      "  Model: 256D, 4L, 4H\n",
      "  Parameters: ~3.7M\n",
      "  Training steps: 100\n",
      "  Effective batch size: 8\n",
      "  Learning rate: 0.0003\n",
      "  Sequence length: 2048\n",
      "============================================================\n",
      "Training failed with error: {'steps': 100, 'batch_size': 4, 'learning_rate': 0.0003, 'max_sequence_length': 2048, 'betas': (0.9, 0.95), 'weight_decay': 0.01, 'grad_clip_norm': 1.0, 'serialization_mode': 'row', 'pad_token_id': 0, 'ignore_index': 0, 'save_every': 50, 'eval_every': 1000, 'use_amp': True, 'grad_accumulation_steps': 2} is not in list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/3w/z9ryb6gj05n_x375lp0r_81r0000gn/T/ipykernel_3819/3394006310.py\", line 37, in <module>\n",
      "    list(TRAINING_CONFIGS.values()).index(training_cfg)\n",
      "ValueError: {'steps': 100, 'batch_size': 4, 'learning_rate': 0.0003, 'max_sequence_length': 2048, 'betas': (0.9, 0.95), 'weight_decay': 0.01, 'grad_clip_norm': 1.0, 'serialization_mode': 'row', 'pad_token_id': 0, 'ignore_index': 0, 'save_every': 50, 'eval_every': 1000, 'use_amp': True, 'grad_accumulation_steps': 2} is not in list\n"
     ]
    }
   ],
   "source": [
    "# Start training with centralized configurations\n",
    "import time\n",
    "\n",
    "print(\"Starting TinyLM training with centralized configuration...\")\n",
    "print(f\"Model will be saved to: {TRAINING_CONFIG['model_dir']}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display final configuration summary\n",
    "model_cfg = TRAINING_CONFIG[\"model_config\"]\n",
    "training_cfg = TRAINING_CONFIG[\"training_config\"]\n",
    "selected_model_size = TRAINING_CONFIG[\"selected_model_size\"]\n",
    "selected_training_profile = TRAINING_CONFIG[\"selected_training_profile\"]\n",
    "\n",
    "print(\"CONFIGURATION SUMMARY:\")\n",
    "print(f\"  Model size: {selected_model_size}\")\n",
    "print(f\"  Training profile: {selected_training_profile}\")\n",
    "print(f\"  Model: {model_cfg['d_model']}D, {model_cfg['n_layers']}L, {model_cfg['n_heads']}H\")\n",
    "print(f\"  Parameters: ~{estimate_model_parameters(model_cfg)/1e6:.1f}M\")\n",
    "print(f\"  Training steps: {training_cfg['steps']:,}\")\n",
    "print(f\"  Effective batch size: {training_cfg['batch_size'] * training_cfg['grad_accumulation_steps']}\")\n",
    "print(f\"  Learning rate: {training_cfg['learning_rate']}\")\n",
    "print(f\"  Sequence length: {training_cfg['max_sequence_length']}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Call the training function with dynamically selected configurations\n",
    "    train(\n",
    "        model_dir=TRAINING_CONFIG[\"model_dir\"],\n",
    "        data_path=TRAINING_CONFIG[\"data_path\"],\n",
    "        # Pass individual parameters\n",
    "        steps=training_cfg[\"steps\"],\n",
    "        bs=training_cfg[\"batch_size\"],\n",
    "        lr=training_cfg[\"learning_rate\"],\n",
    "        d_model=model_cfg[\"d_model\"],\n",
    "        # Use the dynamically selected profiles from previous cell\n",
    "        model_size=selected_model_size,\n",
    "        training_profile=selected_training_profile,\n",
    "        resume_from_checkpoint=True, # save checkpoints in ARC-AGI-2/models/tinylm_checkpoints/\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"Training completed successfully!\")\n",
    "    print(f\"  Configuration used: {selected_model_size} model with {selected_training_profile} profile\")\n",
    "    print(f\"  Training time: {training_time/60:.1f} minutes\")\n",
    "    print(f\"  Models saved to: {TRAINING_CONFIG['model_dir']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Training failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a851624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint Management Utilities # Hopefully not needed often\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "def backup_checkpoint(model_dir, backup_name=None):\n",
    "    \"\"\"Create a backup of the current best.pt checkpoint.\"\"\"\n",
    "    model_path = Path(model_dir)\n",
    "    best_checkpoint = model_path / \"best.pt\"\n",
    "    \n",
    "    if not best_checkpoint.exists():\n",
    "        print(\"No best.pt checkpoint found to backup\")\n",
    "        return False\n",
    "    \n",
    "    if backup_name is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        backup_name = f\"best_backup_{timestamp}.pt\"\n",
    "    \n",
    "    backup_path = model_path / backup_name\n",
    "    shutil.copy2(best_checkpoint, backup_path)\n",
    "    print(f\"Checkpoint backed up to: {backup_path}\")\n",
    "    return True\n",
    "\n",
    "def clear_checkpoints(model_dir, keep_best=True, confirm=True):\n",
    "    \"\"\"Clear all checkpoints. Optionally keep best.pt.\"\"\"\n",
    "    model_path = Path(model_dir)\n",
    "    \n",
    "    if confirm:\n",
    "        response = input(\"Are you sure you want to clear checkpoints? (yes/no): \")\n",
    "        if response.lower() != 'yes':\n",
    "            print(\"Operation cancelled\")\n",
    "            return\n",
    "    \n",
    "    # Find all checkpoint files\n",
    "    checkpoint_files = list(model_path.glob(\"*.pt\"))\n",
    "    \n",
    "    cleared_count = 0\n",
    "    for checkpoint_file in checkpoint_files:\n",
    "        if keep_best and checkpoint_file.name == \"best.pt\":\n",
    "            continue\n",
    "        \n",
    "        checkpoint_file.unlink()\n",
    "        print(f\"Removed: {checkpoint_file.name}\")\n",
    "        cleared_count += 1\n",
    "    \n",
    "    print(f\"Cleared {cleared_count} checkpoint files\")\n",
    "\n",
    "def reset_training_from_scratch(model_dir):\n",
    "    \"\"\"Reset training to start completely from scratch.\"\"\"\n",
    "    print(\"Resetting training to start from scratch...\")\n",
    "    \n",
    "    # Backup current best checkpoint if it exists\n",
    "    if backup_checkpoint(model_dir):\n",
    "        print(\"Current best checkpoint has been backed up\")\n",
    "    \n",
    "    # Clear all checkpoints\n",
    "    clear_checkpoints(model_dir, keep_best=False, confirm=False)\n",
    "    \n",
    "    print(\"Training reset complete. Next training run will start from scratch.\")\n",
    "\n",
    "# Checkpoint management options\n",
    "print(\"Checkpoint Management Options:\")\n",
    "print(\"1. Check checkpoint status (run the cell above)\")  \n",
    "print(\"2. Backup current best.pt: backup_checkpoint(TRAINING_CONFIG['model_dir'])\")  \n",
    "print(\"3. Clear old checkpoints: clear_checkpoints(TRAINING_CONFIG['model_dir'])\")\n",
    "print(\"4. Reset to start from scratch: reset_training_from_scratch(TRAINING_CONFIG['model_dir'])\")\n",
    "print(\"\\nUncomment and run the desired operation below:\")\n",
    "\n",
    "# Example usage (uncomment to use):\n",
    "# backup_checkpoint(TRAINING_CONFIG['model_dir'])\n",
    "# clear_checkpoints(TRAINING_CONFIG['model_dir'], keep_best=True)  \n",
    "# reset_training_from_scratch(TRAINING_CONFIG['model_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45114c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training results\n",
    "import os\n",
    "\n",
    "model_dir = Path(TRAINING_CONFIG[\"model_dir\"])\n",
    "if model_dir.exists():\n",
    "    print(\"Training output files:\")\n",
    "    for file in sorted(model_dir.iterdir()):\n",
    "        if file.is_file():\n",
    "            size_mb = file.stat().st_size / (1024 * 1024)\n",
    "            print(f\"  {file.name}: {size_mb:.1f} MB\")\n",
    "    \n",
    "    # Check if best model exists\n",
    "    best_model = model_dir / \"best.pt\"\n",
    "    if best_model.exists():\n",
    "        print(f\"\\nBest model saved: {best_model}\")\n",
    "        # Load and display best model info\n",
    "        try:\n",
    "            import torch\n",
    "            checkpoint = torch.load(best_model, map_location='cpu')\n",
    "            if 'loss' in checkpoint:\n",
    "                print(f\"   Best loss: {checkpoint['loss']:.4f}\")\n",
    "            if 'cfg' in checkpoint:\n",
    "                cfg = checkpoint['cfg']\n",
    "                print(f\"   Model config: {cfg}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Could not load model info: {e}\")\n",
    "else:\n",
    "    print(\"No training output found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf257dd",
   "metadata": {},
   "source": [
    "## Model Testing (Optional)\n",
    "\n",
    "Test the trained model with a simple forward pass to ensure it's working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8432fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and test the best model\n",
    "model_path = model_dir / \"best.pt\"\n",
    "\n",
    "if model_path.exists():\n",
    "    print(\"Testing the trained model...\")\n",
    "    \n",
    "    try:\n",
    "        # Load the model\n",
    "        checkpoint = torch.load(model_path, map_location='cpu')\n",
    "        \n",
    "        # Recreate the model using centralized config\n",
    "        from arc.models.tiny_lm import TinyLM, TinyLMConfig\n",
    "        from arc.utils.constants import VOCAB_SIZE\n",
    "        \n",
    "        # Use the same config that was used for training\n",
    "        cfg = TinyLMConfig(**checkpoint['cfg'])\n",
    "        model = TinyLM(cfg)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        \n",
    "        # Move to GPU if available\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        print(f\"Model loaded successfully on {device}\")\n",
    "        \n",
    "        # Display model info\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"Model Statistics:\")\n",
    "        print(f\"  Total parameters: {total_params:,}\")\n",
    "        print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"  Model size: ~{total_params * 4 / 1e6:.1f} MB\")\n",
    "        print(f\"  Architecture: {cfg.d_model}D x {cfg.n_layers}L x {cfg.n_heads}H\")\n",
    "        \n",
    "        # Test with dummy input using the trained sequence length\n",
    "        batch_size = 4\n",
    "        seq_len = min(64, cfg.max_len)  # Use shorter sequence for testing\n",
    "        dummy_input = torch.randint(0, VOCAB_SIZE, (batch_size, seq_len)).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(dummy_input)\n",
    "            print(f\"\\nForward Pass Test:\")\n",
    "            print(f\"  Input shape: {dummy_input.shape}\")\n",
    "            print(f\"  Output shape: {output.shape}\")\n",
    "            print(f\"  Output range: [{output.min():.3f}, {output.max():.3f}]\")\n",
    "            print(f\"  Output mean: {output.mean():.3f}\")\n",
    "            print(f\"  Model is working correctly!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Model testing failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"No trained model found to test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917bf76c",
   "metadata": {},
   "source": [
    "## Download Trained Models\n",
    "\n",
    "Download the trained models to your local machine or save to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59942d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Download files directly (in Colab)\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "if model_dir.exists():\n",
    "    # Create a zip file of all models\n",
    "    zip_path = \"tinylm_models.zip\"\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "        for file in model_dir.iterdir():\n",
    "            if file.is_file():\n",
    "                zipf.write(file, file.name)\n",
    "    \n",
    "    print(f\"Created zip file: {zip_path}\")\n",
    "    print(\"Downloading...\")\n",
    "    files.download(zip_path)\n",
    "else:\n",
    "    print(\"No models to download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec53b995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Save to Google Drive (uncomment to use)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# \n",
    "# # Copy models to Google Drive\n",
    "# import shutil\n",
    "# drive_path = \"/content/drive/MyDrive/TinyLM_Models\"\n",
    "# if model_dir.exists():\n",
    "#     shutil.copytree(model_dir, drive_path, dirs_exist_ok=True)\n",
    "#     print(f\"Models saved to Google Drive: {drive_path}\")\n",
    "\n",
    "print(\"Training notebook complete!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY:\")\n",
    "\n",
    "# Display final configuration that was used\n",
    "final_model_cfg = TRAINING_CONFIG[\"model_config\"]\n",
    "final_training_cfg = TRAINING_CONFIG[\"training_config\"]\n",
    "\n",
    "print(f\"Model: {final_model_cfg['d_model']}D x {final_model_cfg['n_layers']}L x {final_model_cfg['n_heads']}H\")\n",
    "print(f\"Parameters: ~{estimate_model_parameters(final_model_cfg)/1e6:.1f}M\")\n",
    "print(f\"Training steps: {final_training_cfg['steps']:,}\")\n",
    "print(f\"Batch size: {final_training_cfg['batch_size']} (effective: {final_training_cfg['batch_size'] * final_training_cfg['grad_accumulation_steps']})\")\n",
    "print(f\"Sequence length: {final_training_cfg['max_sequence_length']}\")\n",
    "print(f\"Learning rate: {final_training_cfg['learning_rate']}\")\n",
    "print(f\"Models saved to: {TRAINING_CONFIG['model_dir']}\")\n",
    "\n",
    "print(f\"\\nGPU Optimizations Used:\")\n",
    "print(f\"  - Gradient accumulation: {final_training_cfg['grad_accumulation_steps']} steps\")\n",
    "print(f\"  - Mixed precision: {'Yes' if final_training_cfg['use_amp'] else 'No'}\")\n",
    "print(f\"  - Gradient clipping: {final_training_cfg['grad_clip_norm']}\")\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(\"   - Centralized configuration system implemented\")\n",
    "print(\"   - GPU-aware automatic config selection\")\n",
    "print(\"   - Implement proper data loading (arc.io.load_task)\")\n",
    "print(\"   - Implement tokenization (arc.serialize.pack_example)\")\n",
    "print(\"   - Run evaluation on the trained model\")\n",
    "print(\"   - Experiment with different hyperparameters\")\n",
    "print(\"   - Try larger models if GPU memory allows\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
