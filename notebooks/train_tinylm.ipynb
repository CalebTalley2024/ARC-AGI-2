{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce1a54c",
   "metadata": {},
   "source": [
    "# TinyLM Training on Google Colab\n",
    "\n",
    "## This notebook trains a TinyLM model on ARC-AGI data using Google Colab's GPU resources.\n",
    "\n",
    "### Setup Instructions:\n",
    "1. Upload this notebook to Google Colab\n",
    "2. Enable GPU runtime: Runtime → Change runtime type → GPU (T4/V100)\n",
    "3. Run all cells in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e157151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability and install requirements\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Please enable GPU runtime in Colab!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f4442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository from GitHub\n",
    "!git clone https://github.com/CalebTalley2024/ARC-AGI-2.git\n",
    "%cd ARC-AGI-2\n",
    "# Checkout specific branch for consistency\n",
    "!git checkout vedant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e43d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install exact package versions for reproducibility\n",
    "!pip install --quiet \\\n",
    "    numpy==1.24.4 \\\n",
    "    matplotlib==3.7.5 \\\n",
    "    pandas==2.0.3 \\\n",
    "    scipy==1.10.1 \\\n",
    "    scikit-learn==1.3.2 \\\n",
    "    torch==2.2.2 \\\n",
    "    torchvision==0.17.2 \\\n",
    "    torchaudio==2.2.2 \\\n",
    "    transformers==4.46.3 \\\n",
    "    huggingface-hub==0.36.0 \\\n",
    "    tokenizers==0.20.3 \\\n",
    "    safetensors==0.5.3 \\\n",
    "    seaborn==0.13.2 \\\n",
    "    plotly==6.4.0 \\\n",
    "    tqdm==4.67.1 \\\n",
    "    pyyaml==6.0.3 \\\n",
    "    requests==2.32.4 \\\n",
    "    packaging==25.0 \\\n",
    "    jsonschema==4.23.0 \\\n",
    "    fastjsonschema==2.21.2 \\\n",
    "    jinja2==3.1.6 \\\n",
    "    markupsafe==2.1.5 \\\n",
    "    urllib3==2.2.3 \\\n",
    "    certifi==2025.10.5 \\\n",
    "    charset-normalizer==3.4.4 \\\n",
    "    idna==3.11 \\\n",
    "    python-dateutil==2.9.0.post0 \\\n",
    "    pytz==2025.2 \\\n",
    "    tzdata==2025.2 \\\n",
    "    six==1.17.0 \\\n",
    "    setuptools==75.3.2\n",
    "\n",
    "# Install the package in development mode\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb96a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = Path.cwd()\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import the training function and centralized constants\n",
    "from arc.models.train import train\n",
    "from arc.models.tiny_lm import TinyLMConfig\n",
    "from arc.utils.constants import (\n",
    "    MODEL_CONFIGS, \n",
    "    TRAINING_CONFIGS, \n",
    "    get_matched_configs,\n",
    "    estimate_model_parameters\n",
    ")\n",
    "\n",
    "print(\"Successfully imported training modules and centralized constants\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Available model sizes: {list(MODEL_CONFIGS.keys())}\")\n",
    "print(f\"Available training profiles: {list(TRAINING_CONFIGS.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a707323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data directory structure\n",
    "data_dir = project_root / \"data\"\n",
    "print(\"Data directory contents:\")\n",
    "if data_dir.exists():\n",
    "    for item in data_dir.iterdir():\n",
    "        print(f\"  {item.name}\")\n",
    "        if item.is_dir():\n",
    "            for subitem in item.iterdir():\n",
    "                print(f\"    {subitem.name}\")\n",
    "                if subitem.name == \"arc\" and subitem.is_dir():\n",
    "                    for arcitem in subitem.iterdir():\n",
    "                        print(f\"      {arcitem.name}\")\n",
    "else:\n",
    "    print(\"  WARNING: Data directory not found!\")\n",
    "\n",
    "# Check if training data exists\n",
    "training_path = data_dir / \"raw\" / \"arc\" / \"training\"\n",
    "eval_path = data_dir / \"raw\" / \"arc\" / \"evaluation\"\n",
    "\n",
    "print(f\"\\nTraining data exists: {training_path.exists()}\")\n",
    "print(f\"Evaluation data exists: {eval_path.exists()}\")\n",
    "\n",
    "if training_path.exists():\n",
    "    training_files = list(training_path.glob(\"*.json\"))\n",
    "    print(f\"Number of training files: {len(training_files)}\")\n",
    "    \n",
    "if eval_path.exists():\n",
    "    eval_files = list(eval_path.glob(\"*.json\"))\n",
    "    print(f\"Number of evaluation files: {len(eval_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39402a9",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "Configure the training parameters. Adjust these based on your needs and available GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cfde04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU-aware dynamic configuration selection\n",
    "def select_optimal_configs(gpu_memory_gb):\n",
    "    \"\"\"Select optimal model and training configs based on GPU memory.\"\"\"\n",
    "    \n",
    "    print(f\"GPU Memory: {gpu_memory_gb:.1f} GB\")\n",
    "    \n",
    "    # Select configs based on GPU memory\n",
    "    if gpu_memory_gb < 4:\n",
    "        model_size = 'tiny'\n",
    "        training_profile = 'debug'\n",
    "        print(\"Low GPU memory detected - using minimal config for testing\")\n",
    "    elif gpu_memory_gb < 8:\n",
    "        model_size = 'tiny'\n",
    "        training_profile = 'small_gpu'\n",
    "        print(\"Small GPU detected - using tiny model with memory optimization\")\n",
    "    elif gpu_memory_gb < 16:\n",
    "        model_size = 'small'\n",
    "        training_profile = 'medium_gpu'\n",
    "        print(\"Medium GPU detected - using small model\")\n",
    "    else:\n",
    "        model_size = 'medium'\n",
    "        training_profile = 'large_gpu'\n",
    "        print(\"Large GPU detected - using medium model for best performance\")\n",
    "    \n",
    "    # Get matched configurations\n",
    "    model_config, training_config = get_matched_configs(model_size, training_profile)\n",
    "    \n",
    "    # Display configuration info\n",
    "    param_count = estimate_model_parameters(model_config)\n",
    "    effective_batch_size = training_config['batch_size'] * training_config['grad_accumulation_steps']\n",
    "    \n",
    "    print(f\"\\nSelected Configuration:\")\n",
    "    print(f\"  Model: {model_size} ({param_count/1e6:.1f}M parameters)\")\n",
    "    print(f\"  Training profile: {training_profile}\")\n",
    "    print(f\"  Batch size: {training_config['batch_size']} (effective: {effective_batch_size})\")\n",
    "    print(f\"  Max sequence length: {training_config['max_sequence_length']}\")\n",
    "    print(f\"  Gradient accumulation: {training_config['grad_accumulation_steps']} steps\")\n",
    "    \n",
    "    return model_config, training_config\n",
    "\n",
    "# Check GPU and select optimal configuration\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    model_config, training_config = select_optimal_configs(gpu_memory)\n",
    "else:\n",
    "    print(\"No GPU detected! Using CPU-friendly minimal config\")\n",
    "    model_config, training_config = get_matched_configs('tiny', 'debug')\n",
    "\n",
    "# Training configuration for this session\n",
    "TRAINING_CONFIG = {\n",
    "    # Data paths\n",
    "    \"data_path\": str(data_dir / \"raw\" / \"arc\" / \"training\"),\n",
    "    \"model_dir\": \"./models/tinylm_checkpoints\",\n",
    "    \n",
    "    # Use centralized configurations\n",
    "    \"model_config\": model_config,\n",
    "    \"training_config\": training_config,\n",
    "}\n",
    "\n",
    "print(\"\\nFinal Training Configuration:\")\n",
    "print(f\"  Data path: {TRAINING_CONFIG['data_path']}\")\n",
    "print(f\"  Model dir: {TRAINING_CONFIG['model_dir']}\")\n",
    "print(f\"  Steps: {training_config['steps']:,}\")\n",
    "print(f\"  Learning rate: {training_config['learning_rate']}\")\n",
    "print(f\"  Weight decay: {training_config['weight_decay']}\")\n",
    "print(f\"  Use AMP: {training_config['use_amp']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6290dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path(TRAINING_CONFIG[\"model_dir\"])\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Created output directory: {output_dir}\")\n",
    "\n",
    "# Test model creation with centralized config\n",
    "print(\"\\nTesting model creation with selected configuration...\")\n",
    "try:\n",
    "    from arc.utils.constants import VOCAB_SIZE\n",
    "    print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "    \n",
    "    # Create model with selected configuration\n",
    "    model_cfg = TRAINING_CONFIG[\"model_config\"]\n",
    "    test_config = TinyLMConfig(**model_cfg)\n",
    "    print(f\"Model config created successfully\")\n",
    "    print(f\"  Architecture: {test_config.d_model}D, {test_config.n_layers} layers, {test_config.n_heads} heads\")\n",
    "    print(f\"  Parameters: ~{estimate_model_parameters(model_cfg)/1e6:.1f}M\")\n",
    "    print(f\"  Max sequence length: {test_config.max_len}\")\n",
    "    \n",
    "    # Display memory usage estimate\n",
    "    param_size_mb = estimate_model_parameters(model_cfg) * 4 / 1e6  # 4 bytes per float32\n",
    "    training_cfg = TRAINING_CONFIG[\"training_config\"]\n",
    "    batch_memory_mb = (training_cfg['batch_size'] * training_cfg['max_sequence_length'] * \n",
    "                      model_cfg['d_model'] * 4) / 1e6\n",
    "    \n",
    "    print(f\"\\nMemory Estimates:\")\n",
    "    print(f\"  Model size: ~{param_size_mb:.0f} MB\")\n",
    "    print(f\"  Batch memory: ~{batch_memory_mb:.0f} MB\")\n",
    "    print(f\"  Total training memory: ~{param_size_mb + batch_memory_mb*3:.0f} MB (estimated)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating model: {e}\")\n",
    "    print(\"This might indicate missing implementations in serialize module\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a8e59c",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "\n",
    "**Note:** The current implementation has placeholder functions for data loading. This will train on empty data but demonstrates the training loop. You'll need to implement proper data loading for actual training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d7faf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training with centralized configurations\n",
    "import time\n",
    "\n",
    "print(\"Starting TinyLM training with centralized configuration...\")\n",
    "print(f\"Model will be saved to: {TRAINING_CONFIG['model_dir']}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display final configuration summary\n",
    "model_cfg = TRAINING_CONFIG[\"model_config\"]\n",
    "training_cfg = TRAINING_CONFIG[\"training_config\"]\n",
    "\n",
    "print(\"CONFIGURATION SUMMARY:\")\n",
    "print(f\"  Model: {model_cfg['d_model']}D, {model_cfg['n_layers']}L, {model_cfg['n_heads']}H\")\n",
    "print(f\"  Parameters: ~{estimate_model_parameters(model_cfg)/1e6:.1f}M\")\n",
    "print(f\"  Training steps: {training_cfg['steps']:,}\")\n",
    "print(f\"  Effective batch size: {training_cfg['batch_size'] * training_cfg['grad_accumulation_steps']}\")\n",
    "print(f\"  Learning rate: {training_cfg['learning_rate']}\")\n",
    "print(f\"  Sequence length: {training_cfg['max_sequence_length']}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Call the training function with centralized configs\n",
    "    train(\n",
    "        model_dir=TRAINING_CONFIG[\"model_dir\"],\n",
    "        data_path=TRAINING_CONFIG[\"data_path\"],\n",
    "        # Pass individual model config parameters for compatibility\n",
    "        steps=training_cfg[\"steps\"],\n",
    "        bs=training_cfg[\"batch_size\"],\n",
    "        lr=training_cfg[\"learning_rate\"],\n",
    "        d_model=model_cfg[\"d_model\"],\n",
    "        # n_layers=model_cfg[\"n_layers\"],\n",
    "        # n_heads=model_cfg[\"n_heads\"],\n",
    "        # Use training profile for gradient accumulation\n",
    "        training_profile=list(TRAINING_CONFIGS.keys())[\n",
    "            list(TRAINING_CONFIGS.values()).index(training_cfg)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"Training completed successfully!\")\n",
    "    print(f\"  Training time: {training_time/60:.1f} minutes\")\n",
    "    print(f\"Models saved to: {TRAINING_CONFIG['model_dir']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Training failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45114c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training results\n",
    "import os\n",
    "\n",
    "model_dir = Path(TRAINING_CONFIG[\"model_dir\"])\n",
    "if model_dir.exists():\n",
    "    print(\"Training output files:\")\n",
    "    for file in sorted(model_dir.iterdir()):\n",
    "        if file.is_file():\n",
    "            size_mb = file.stat().st_size / (1024 * 1024)\n",
    "            print(f\"  {file.name}: {size_mb:.1f} MB\")\n",
    "    \n",
    "    # Check if best model exists\n",
    "    best_model = model_dir / \"best.pt\"\n",
    "    if best_model.exists():\n",
    "        print(f\"\\nBest model saved: {best_model}\")\n",
    "        # Load and display best model info\n",
    "        try:\n",
    "            import torch\n",
    "            checkpoint = torch.load(best_model, map_location='cpu')\n",
    "            if 'loss' in checkpoint:\n",
    "                print(f\"   Best loss: {checkpoint['loss']:.4f}\")\n",
    "            if 'cfg' in checkpoint:\n",
    "                cfg = checkpoint['cfg']\n",
    "                print(f\"   Model config: {cfg}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Could not load model info: {e}\")\n",
    "else:\n",
    "    print(\"No training output found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf257dd",
   "metadata": {},
   "source": [
    "## Model Testing (Optional)\n",
    "\n",
    "Test the trained model with a simple forward pass to ensure it's working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8432fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and test the best model\n",
    "model_path = model_dir / \"best.pt\"\n",
    "\n",
    "if model_path.exists():\n",
    "    print(\"Testing the trained model...\")\n",
    "    \n",
    "    try:\n",
    "        # Load the model\n",
    "        checkpoint = torch.load(model_path, map_location='cpu')\n",
    "        \n",
    "        # Recreate the model using centralized config\n",
    "        from arc.models.tiny_lm import TinyLM, TinyLMConfig\n",
    "        from arc.utils.constants import VOCAB_SIZE\n",
    "        \n",
    "        # Use the same config that was used for training\n",
    "        cfg = TinyLMConfig(**checkpoint['cfg'])\n",
    "        model = TinyLM(cfg)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        \n",
    "        # Move to GPU if available\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        print(f\"Model loaded successfully on {device}\")\n",
    "        \n",
    "        # Display model info\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"Model Statistics:\")\n",
    "        print(f\"  Total parameters: {total_params:,}\")\n",
    "        print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"  Model size: ~{total_params * 4 / 1e6:.1f} MB\")\n",
    "        print(f\"  Architecture: {cfg.d_model}D x {cfg.n_layers}L x {cfg.n_heads}H\")\n",
    "        \n",
    "        # Test with dummy input using the trained sequence length\n",
    "        batch_size = 4\n",
    "        seq_len = min(64, cfg.max_len)  # Use shorter sequence for testing\n",
    "        dummy_input = torch.randint(0, VOCAB_SIZE, (batch_size, seq_len)).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(dummy_input)\n",
    "            print(f\"\\nForward Pass Test:\")\n",
    "            print(f\"  Input shape: {dummy_input.shape}\")\n",
    "            print(f\"  Output shape: {output.shape}\")\n",
    "            print(f\"  Output range: [{output.min():.3f}, {output.max():.3f}]\")\n",
    "            print(f\"  Output mean: {output.mean():.3f}\")\n",
    "            print(f\"  Model is working correctly!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Model testing failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"No trained model found to test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917bf76c",
   "metadata": {},
   "source": [
    "## Download Trained Models\n",
    "\n",
    "Download the trained models to your local machine or save to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59942d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Download files directly (in Colab)\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "if model_dir.exists():\n",
    "    # Create a zip file of all models\n",
    "    zip_path = \"tinylm_models.zip\"\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "        for file in model_dir.iterdir():\n",
    "            if file.is_file():\n",
    "                zipf.write(file, file.name)\n",
    "    \n",
    "    print(f\"Created zip file: {zip_path}\")\n",
    "    print(\"Downloading...\")\n",
    "    files.download(zip_path)\n",
    "else:\n",
    "    print(\"No models to download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec53b995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Save to Google Drive (uncomment to use)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# \n",
    "# # Copy models to Google Drive\n",
    "# import shutil\n",
    "# drive_path = \"/content/drive/MyDrive/TinyLM_Models\"\n",
    "# if model_dir.exists():\n",
    "#     shutil.copytree(model_dir, drive_path, dirs_exist_ok=True)\n",
    "#     print(f\"Models saved to Google Drive: {drive_path}\")\n",
    "\n",
    "print(\"Training notebook complete!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY:\")\n",
    "\n",
    "# Display final configuration that was used\n",
    "final_model_cfg = TRAINING_CONFIG[\"model_config\"]\n",
    "final_training_cfg = TRAINING_CONFIG[\"training_config\"]\n",
    "\n",
    "print(f\"Model: {final_model_cfg['d_model']}D x {final_model_cfg['n_layers']}L x {final_model_cfg['n_heads']}H\")\n",
    "print(f\"Parameters: ~{estimate_model_parameters(final_model_cfg)/1e6:.1f}M\")\n",
    "print(f\"Training steps: {final_training_cfg['steps']:,}\")\n",
    "print(f\"Batch size: {final_training_cfg['batch_size']} (effective: {final_training_cfg['batch_size'] * final_training_cfg['grad_accumulation_steps']})\")\n",
    "print(f\"Sequence length: {final_training_cfg['max_sequence_length']}\")\n",
    "print(f\"Learning rate: {final_training_cfg['learning_rate']}\")\n",
    "print(f\"Models saved to: {TRAINING_CONFIG['model_dir']}\")\n",
    "\n",
    "print(f\"\\nGPU Optimizations Used:\")\n",
    "print(f\"  - Gradient accumulation: {final_training_cfg['grad_accumulation_steps']} steps\")\n",
    "print(f\"  - Mixed precision: {'Yes' if final_training_cfg['use_amp'] else 'No'}\")\n",
    "print(f\"  - Gradient clipping: {final_training_cfg['grad_clip_norm']}\")\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(\"   - Centralized configuration system implemented\")\n",
    "print(\"   - GPU-aware automatic config selection\")\n",
    "print(\"   - Implement proper data loading (arc.io.load_task)\")\n",
    "print(\"   - Implement tokenization (arc.serialize.pack_example)\")\n",
    "print(\"   - Run evaluation on the trained model\")\n",
    "print(\"   - Experiment with different hyperparameters\")\n",
    "print(\"   - Try larger models if GPU memory allows\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
